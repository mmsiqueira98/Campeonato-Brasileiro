{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8+LoVx8Td9b7rBsyfEbYY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmsiqueira98/Campeonato-Brasileiro/blob/main/ESPN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#WEB SCRAPING ESPN PARA DADOS ESTATÍSTICOS FUTEBOL\n",
        "\n",
        "O objetivo é fazer o scraping do site ESPN com os dados dos times de futebol, para"
      ],
      "metadata": {
        "id": "4iggB3URc19T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pacotes Bibliotecas e links"
      ],
      "metadata": {
        "id": "uPEJh3oodFGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUGHvqJkeqfZ",
        "outputId": "1d5824d4-7ac6-4646-867a-1034bd6d76ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUCL2GORc1D7"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "from requests import HTTPError\n",
        "from xml.dom import minidom\n",
        "import pandas as pd\n",
        "from unidecode import unidecode\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from typing import List\n",
        "from bs4.element import Tag"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scrapping\n",
        "\n",
        "Como observei na estrutura do site da ESPN, os times são categorizados pela seguinte estrutura: \n",
        "\n",
        "a = **https://www.espn.com/soccer/team/_/id/6273/gremio**\n",
        "\n",
        "b = **https://www.espn.com/soccer/team/_/id/6273**\n",
        "\n",
        "  o valor que vem depois de ID é o que eu preciso, na realidade a descrição do time, como nesse exemplo está gremio, pouco me importa. já que se eu colocar o link até o valor de 6273, serei entregue a pagina responsável pelo grêmio.\n",
        "Logo, a = b"
      ],
      "metadata": {
        "id": "sG2Ypz7MdHkA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A primeira coisa que preciso fazer é definir essa função **crawl_website** que será responsável por fazer o scrapping dos sites que eu for utilizar futuramente"
      ],
      "metadata": {
        "id": "3sE8xO585NMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crawl_website(url: str, archive: str) -> str: \n",
        "  try: \n",
        "    resposta = requests.get(url)\n",
        "    \n",
        "  \n",
        "  except HTTPError as exc: \n",
        "    print(exc)\n",
        "  \n",
        "  else: \n",
        "    b = resposta.text\n",
        "    soup = BeautifulSoup(b, 'html.parser')\n",
        "    with open(f'{archive}.html', 'w') as file:\n",
        "      file.write(soup.prettify())\n",
        "  \n",
        "  pagina = BeautifulSoup(open(f'{archive}.html',mode='r'),'html.parser')\n",
        "  return pagina"
      ],
      "metadata": {
        "id": "VbQ34t-Yey2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_all(elementos: List[Tag], letra: str, classe: str, objeto: str):\n",
        "  elementos = elementos.find_all(letra, class_=classe)\n",
        "  lista = []\n",
        "  for elemento in elementos: \n",
        "    valor_href = elemento[objeto]\n",
        "    lista.append(valor_href)\n",
        "\n",
        "  return lista"
      ],
      "metadata": {
        "id": "aijzir9-ND55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora, a URL, onde estão as informações dos times: \n",
        "\n",
        "**https://www.espn.com/soccer/teams/_/league/bra.1**"
      ],
      "metadata": {
        "id": "4MhIs_qu68F1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "times = 'https://www.espn.com/soccer/teams/_/league/bra.1'\n",
        "id_times = crawl_website(url = times, archive = 'BrasileiraoSerieA')\n",
        "lista = find_all(elementos = id_times, letra = 'a', classe = 'AnchorLink', objeto = 'href' )\n"
      ],
      "metadata": {
        "id": "X4DGUFz5oFk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elementos = id_times.find_all('a', class_='AnchorLink')\n",
        "lista = []\n",
        "for elemento in elementos: \n",
        "  valor_href = elemento['href']\n",
        "  lista.append(valor_href)\n",
        "\n"
      ],
      "metadata": {
        "id": "MAjE9UPMrHb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora, eu preciso separar o soccer/team, excluir isso e juntar o resultado em uma lista, sem que haja repetições."
      ],
      "metadata": {
        "id": "oJudttw89fg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "padrao = re.compile(r'/soccer/team/stats/_/id/(.*)')\n",
        "valores_filtrados = []\n",
        "\n",
        "for valor in lista: \n",
        "  resultado = padrao.search(valor)\n",
        "  if resultado: \n",
        "    informacoes = resultado.group(1)\n",
        "    valores_filtrados.append(informacoes)\n",
        "  \n",
        "id_partida = list(set(valores_filtrados))\n",
        "times = [valor.split('/') for valor in valores_filtrados]\n",
        "\n",
        "time = []\n",
        "id = []\n",
        "\n",
        "for i in times: \n",
        "  time.append(i[1])\n",
        "  id.append(i[0])\n",
        "\n",
        "print(time)\n",
        "print(id)"
      ],
      "metadata": {
        "id": "k112ic4AuY8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "year = ['2017', '2018', '2019', '2020', '2021', '2022']\n",
        "time = []\n",
        "id = []\n",
        "\n",
        "for i in times: \n",
        "  time.append(i[1])\n",
        "  id.append(i[0])\n",
        "\n",
        "print(time)\n",
        "print(id)"
      ],
      "metadata": {
        "id": "UF0Ofu9NaVdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ótimo, esse resultado é tudo o que eu preciso para esse momento."
      ],
      "metadata": {
        "id": "080eKHsStBrT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora vamos criar a variável link."
      ],
      "metadata": {
        "id": "qph-P1rfoB3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "link = []\n",
        "\n",
        "for i in id: \n",
        "  for j in year: \n",
        "    a = f'https://www.espn.com/soccer/team/results/_/id/{i}/season/{j}'\n",
        "    link.append(a)\n",
        "\n",
        "print(link)"
      ],
      "metadata": {
        "id": "sN2LIeNMoEAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Por qual motivo eu fiz isso? \n",
        "\n",
        "Bom, com o id do time que está no brasileirão da série A, agora eu posso ir até esse domínio do site: \n",
        "\n",
        "**https://www.espn.com/soccer/team/results/_/id/{id}/season/{year}**\n",
        "\n",
        "onde\n",
        "\n",
        "**id = id do time**\n",
        "\n",
        "e \n",
        "\n",
        "**year = ano do time**\n",
        "\n",
        "O objetivo é rastrear os jogos de cada time de 2017 até 2022. Mas primeiro, iremos realizar o crawling de todos os dados do site que estão correspondendo aos valores relativos a link.\n",
        "\n"
      ],
      "metadata": {
        "id": "yClzA3dq-z1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id_times = []\n",
        "a = 0\n",
        "for i in link: \n",
        "  a+=1\n",
        "  partidas = crawl_website(url = i, archive = f'{a}')"
      ],
      "metadata": {
        "id": "vApqsi-fjnu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feito isso, Temos agora todos os scraping dos sites com os times nos torneios de 2017 até o momento, considerando apenas aqueles que estão na Série A nesse campeonato. Nesses respectivos crawlings realizados nos temos uma informação muito importante conforme está mostrada nesse link: \n",
        "\n",
        "'https://www.espn.com/soccer/matchstats/_/gameId/{informação_desejada}'\n",
        "\n",
        "Essa informação é o GameId, onde contém o resultado de uma partida de futebol entre dois times, o seu resultado e também, em algumas partidas, as estatísticas durante as partidas de futebol, que é o nosso grande objetivo com esse trabalho."
      ],
      "metadata": {
        "id": "hUO00lBz88YH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Realizando o Web Scrapping dos dados obtidos\n",
        "\n",
        "Como foi observado anteriormente, os dados foram enumerados de 0 a n, onde 0 é o primeiro crawling e n é o último. Para sabermos qual é o número de crawling's feitos precisamos apenas ver a quantidade links que tem. Sabendo que são 120 então nos temos 0.html a 119.html. Precisamos realizar o scrapping desses dados temo como alvo: \n",
        "\n",
        " /soccer/matchstats/_/gameId/(Informação_desejada)"
      ],
      "metadata": {
        "id": "In7bb1qi_0Db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "partidas = []\n",
        "for a in range(1, 120): \n",
        "  pagina = BeautifulSoup(open(f'/content/{a}.html',mode='r'),'html.parser')\n",
        "\n",
        "  b = find_all(elementos=pagina, letra='a', classe = 'AnchorLink', objeto = 'href')\n",
        "  padrao = re.compile(r'/gameId/(\\d+)')\n",
        "  valores_filtrados = []\n",
        "\n",
        "  for valor in b: \n",
        "    resultado = padrao.search(valor)\n",
        "    if resultado: \n",
        "      informacoes = resultado.group(1)\n",
        "      valores_filtrados.append(informacoes)\n",
        "      partidas.append(informacoes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isWMylurjnRv",
        "outputId": "2f90d951-ff46-4377-89d3-6a370732066f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos avaliar a quantidade de dados."
      ],
      "metadata": {
        "id": "7di5Zm-1EbTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "partidas_unicas = list(set(partidas))\n",
        "print(len(partidas_unicas)) \n",
        "print(partidas_unicas)"
      ],
      "metadata": {
        "id": "aq54PdsaftqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos salvar esses arquivos em csv para não precisar realizar o scrapping novamente até aqui."
      ],
      "metadata": {
        "id": "gk7932wbEw6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nome_do_arquivo = 'id_das_partidas.csv'\n",
        "\n",
        "with open (nome_do_arquivo, 'w', newline='') as arquivo: \n",
        "  escritor_csv = csv.writer(arquivo, delimiter=',')\n",
        "  escritor_csv.writerow(['Valores'])\n",
        "\n",
        "  for valor in partidas_unicas: \n",
        "    escritor_csv.writerow([valor])"
      ],
      "metadata": {
        "id": "RQ9pygI4FaSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui eu vou simplesmente criar uma lista para prosseguir com o código, para que sua leitura seja menos confusa"
      ],
      "metadata": {
        "id": "fc3jPZDPc---"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "o que eu tenho em link? Em Link eu tenho todos os times que estão na Série A desse ano (20) e sua trajetória desde 2017. Logo, eu tenho 6 anos e 20 times o que é igual a 120."
      ],
      "metadata": {
        "id": "Fw3HY_yhdn3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Próximos Passos\n",
        "\n",
        "Os próximos passos agora são abrir os sites e filtar aqueles que contém as estatísticas dos times que se confrontaram para que aí sim possamos criar um novo csv com todos esses valores e partir para a Análise de Dados."
      ],
      "metadata": {
        "id": "K8DKfxEdGQi2"
      }
    }
  ]
}